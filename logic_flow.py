import sys
import json
import asyncio
import base64
import re
import time
import unicodedata 
import random
from services import AIServices
from database import db

# --- IMPORT MODULE V·ªÜ TINH ---
try:
    from analyze import VN_VoiceBot_Parallel
    from data_engine import DataEngine
    from strategy_competitor import CompetitorStrategy
    from strategy_low_data import LowDataStrategy
    from strategy_network import NetworkStrategy
except ImportError as e:
    print(f" Thi·∫øu file v·ªá tinh: {e}")
    sys.exit(1)
    
# --- ADAPTER (GI·ªÆ NGUY√äN) ---
class GeminiAdapter:
    def __init__(self, ai_service):
        self.ai_service = ai_service
        self.main_loop = None
    
    def set_main_loop(self, loop):
        self.main_loop = loop

    def generate_content(self, prompt):
        if self.main_loop and self.main_loop.is_running():
            future = asyncio.run_coroutine_threadsafe(
                self.ai_service.chat_gemini_fallback(prompt), 
                self.main_loop
            )
            try: text = future.result(timeout=10)
            except Exception: text = "D·∫°, em nghe r√µ ·∫°."
            return type('obj', (object,), {'text': text})()
        else:
            return type('obj', (object,), {'text': "D·∫° v√¢ng ·∫°."})()

# --- CONTROLLER CH√çNH ---
class TelesalesAgent:
    def __init__(self):
        print("\n" + "="*40)
        print(" [SYSTEM] KH·ªûI ƒê·ªòNG LOGIC v2.8 (FIX STATIC TIMING)")
        print(" [SYSTEM] Ch·∫ø ƒë·ªô: G·ª≠i Metrics tr∆∞·ªõc Text ·ªü m·ªçi k·ªãch b·∫£n")
        print("="*40 + "\n")
        
        self.ai_service = AIServices()
        self.analyzer = VN_VoiceBot_Parallel()
        self.data_engine = DataEngine("test_customer.csv", "product_collection.json")
        self.adapter = GeminiAdapter(self.ai_service)
        
        self.strategies = {
            "ƒê·ªëi th·ªß": CompetitorStrategy(self.adapter, self.data_engine),
            "√çt data": LowDataStrategy(self.adapter, self.data_engine),
            "M·∫°ng ngh·∫Ωn": NetworkStrategy(self.adapter, self.data_engine)
        }
        
        self.sales_data = {}
        self._load_scripts()
        self.sessions = {} 
        
        self.MSG_FALLBACK = "Xin l·ªói kh√°ch h√†ng nh∆∞ng tr∆∞·ªùng h·ª£p n√†y ch∆∞a ƒë∆∞·ª£c t√≠ch h·ª£p trong MVP. Xin h√£y s·ª≠ d·ª•ng nh·ªØng t√¨nh hu·ªëng c√≥ trong k·ªãch b·∫£n cung c·∫•p."
        self.MSG_CLOSING = "C·∫£m ∆°n v√¨ ƒë√£ d√†nh th·ªùi gian ƒë·ªÉ nghe t∆∞ v·∫•n, ƒë·ªôi k·ªπ thu·∫≠t s·∫Ω li√™n h·ªá v·ªõi {pronoun} ƒë·ªÉ t√≠ch h·ª£p g√≥i c∆∞·ªõc sau ·∫°."
        self.MSG_MVP_END = "ƒê√¢y l√† to√†n b·ªô ph·∫ßn MVP c·ªßa \"Quarter Zip.\""

        self.sentence_split_regex = re.compile(r'(?<=[.?!;])\s+')

    def _load_scripts(self):
        try:
            with open('final_voice_scripts.json', 'r', encoding='utf-8') as f:
                scripts = json.load(f)
                for item in scripts:
                    self.sales_data[str(item['customer_id'])] = item
        except: pass

    # --- H√ÄM HELPER ƒê·ªÇ G·ª¨I METRICS NHANH ---
    def _create_metrics_payload(self, stt_latency, start_process_time, sentiment, intent):
        return json.dumps({
            "type": "metrics_update",
            "data": {
                "latency_stt": round(stt_latency, 2),
                "latency_logic": round(time.time() - start_process_time, 2),
                "sentiment": round(sentiment, 2),
                "intent": intent
            }
        }) + "\n"

    # --- H√ÄM X·ª¨ L√ù CH√çNH ---
    async def process_stream(self, customer_id_str, user_audio):
        cid = str(customer_id_str)
        current_loop = asyncio.get_running_loop()
        self.adapter.set_main_loop(current_loop)

        # 1. INIT SESSION
        if cid not in self.sessions or user_audio is None:
            print(f"\n--- NEW SESSION: {cid} ---")
            ctx = self.data_engine.get_full_context(cid)
            cust_data = ctx['customer'] if ctx else {}
            cust_name = cust_data.get('Display_Name', 'Qu√Ω kh√°ch')
            
            gender_raw = cust_data.get('Gender', '').lower().strip()
            if gender_raw in ['nam', 'male', 'trai', 'mr', 'anh']: pronoun = "Anh"
            elif gender_raw in ['nu', 'n·ªØ', 'female', 'g√°i', 'ms', 'mrs', 'ch·ªã']: pronoun = "Ch·ªã"
            else: pronoun = "M√¨nh"
                
            self.sessions[cid] = {
                'step': 1, 
                'history': [], 
                'pronoun': pronoun,
                'start_time': time.time(),
                'detected_intent': "General"
            }
            
            greeting = f"D·∫°, em ch√†o {pronoun} {cust_name}, em l√† nh√¢n vi√™n CSKH VNPT. Em th·∫•y {pronoun.lower()} ƒëang d√πng d·ªãch v·ª• b√™n em, kh√¥ng bi·∫øt qu√° tr√¨nh s·ª≠ d·ª•ng c√≥ ·ªïn ƒë·ªãnh kh√¥ng ·∫°?"
            greeting = self._normalize_pronouns(greeting, pronoun)
            async for chunk in self._stream_text_and_audio(greeting): yield chunk
            return

        session = self.sessions[cid]
        step = session['step']
        current_pronoun = session.get('pronoun', 'M√¨nh')
        detected_intent = session.get('detected_intent', 'General')
    
        # [FILLER] - V·∫´n g·ª≠i tr∆∞·ªõc khi c√≥ logic (ƒê·ªìng h·ªì AI v·∫´n ch·∫°y)
        if user_audio:
            filler_options = [
                "D·∫°, {p} ch·ªù em m·ªôt ch√∫t nh√©.",
                "D·∫° v√¢ng, ƒë·ªÉ em ki·ªÉm tra tr√™n h·ªá th·ªëng gi√∫p {p} ngay ·∫°.",
                "D·∫° em nghe r√µ r·ªìi ·∫°, {p} ƒë·ª£i em x√≠u nh√©.",
                "V√¢ng ·∫°, em ƒëang tra c·ª©u th√¥ng tin cho {p} ƒë√¢y ·∫°.",
                "D·∫°, {p} gi·ªØ m√°y gi√∫p em v√†i gi√¢y nh√©."
            ]
            chosen_template = random.choice(filler_options)
            filler_text = chosen_template.format(p=current_pronoun.lower())
            async for chunk in self._stream_text_and_audio(filler_text): yield chunk

        # 2. STT
        start_stt = time.time()
        user_text = await self.ai_service.speech_to_text(user_audio)
        stt_latency = time.time() - start_stt

        if not user_text:
            async for chunk in self._stream_text_and_audio("Alo, em kh√¥ng nghe r√µ, n√≥i l·∫°i gi√∫p em nh√©?"): yield chunk
            return
        
        user_text = unicodedata.normalize('NFC', user_text)
        print(f" [USER] {user_text}")
        session['history'].append(f"User: {user_text}")
        yield json.dumps({"user_text": user_text}) + "\n"

        bot_reply_accumulated = ""
        end_of_mvp = False
        start_process = time.time()
        sentiment_score = 0.0
        
        # --- CORE LOGIC ---
        triggers_agree = ["ƒë·ªìng √Ω", "ƒëƒÉng k√Ω", "ok", "ch·ªët", "l·∫•y g√≥i n√†y", "ƒë∆∞·ª£c ƒë·∫•y", "nh·∫•t tr√≠"]
        is_agreed = False 
        
        if any(w in user_text.lower() for w in triggers_agree):
            is_agreed = True
            negation_patterns = [r"kh√¥ng\s+.*ƒëƒÉng\s+k√Ω", r"kh√¥ng\s+.*ƒë·ªìng\s+√Ω", r"kh√¥ng\s+.*ch·ªët", r"ch∆∞a\s+.*ƒëƒÉng\s+k√Ω", r"kh√¥ng\s+c·∫ßn", r"kh√¥ng\s+mu·ªën"]
            for p in negation_patterns:
                if re.search(p, user_text.lower()): is_agreed = False; break

        # === NH√ÅNH 1: KH√ÅCH ƒê·ªíNG √ù (CH·ªêT ƒê∆†N) ===
        if is_agreed:
            print(" [ACTION] Kh√°ch ƒë·ªìng √Ω -> Ch·ªët ƒë∆°n.")
            response_text = self.MSG_CLOSING.format(pronoun=current_pronoun)
            end_of_mvp = True; session['step'] = 0
            sentiment_score = 1.0
            
            # [FIX] G·ª≠i Metrics TR∆Ø·ªöC Text
            yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)
            
            response_text = self._normalize_pronouns(response_text, current_pronoun)
            async for chunk in self._stream_text_and_audio(response_text): yield chunk
            bot_reply_accumulated = response_text

        else:
            sentiment_score, _ = self.analyzer.analyze_sentiment(user_text)
            toxic_keywords = ["l·ª´a ƒë·∫£o", "m·∫•t d·∫°y", "bi·∫øn ƒëi", "c√∫t", "h·ªßy ngay", "c·∫Øt ngay", "d·∫πp ngay", "h·ªßy g√≥i", "hu·ª∑ g√≥i", "ch√°n ng·∫•y", "kh√¥ng th·ªÉ ch·ªãu", "b·ª±c m√¨nh", "·ª©c ch·∫ø"]
            is_toxic_confirmed = False
            if any(kw in user_text.lower() for kw in toxic_keywords):
                print(f" [OVERRIDE] Ph√°t hi·ªán t·ª´ kh√≥a TOXIC/H·ª¶Y -> √âp sentiment v·ªÅ -5.0.")
                sentiment_score = -5.0
                is_toxic_confirmed = True

            if not is_toxic_confirmed:
                sales_keywords = ["ƒë·∫Øt", "gi√°", "c∆∞·ªõc", "ti·ªÅn", "so s√°nh", "t·ªëc ƒë·ªô", "dung l∆∞·ª£ng", "nh√† m·∫°ng", "∆∞u ƒë√£i", "g√≥i", "l√Ω do"]
                if any(kw in user_text.lower() for kw in sales_keywords): sentiment_score = 0.5

            # === NH√ÅNH 2: TOXIC / HANDOVER ===
            if sentiment_score <= -0.8:
                print(f" [ALERT] Toxic/Hu·ª∑ -> Chuy·ªÉn t∆∞ v·∫•n vi√™n.")
                response_text = f"D·∫°, em th√†nh th·∫≠t xin l·ªói v√¨ tr·∫£i nghi·ªám ch∆∞a t·ªët c·ªßa {current_pronoun.lower()}. V·∫•n ƒë·ªÅ n√†y v∆∞·ª£t qu√° th·∫©m quy·ªÅn c·ªßa em, em s·∫Ω n·ªëi m√°y ngay t·ªõi chuy√™n vi√™n c·∫•p cao ƒë·ªÉ h·ªó tr·ª£ {current_pronoun.lower()} x·ª≠ l√Ω y√™u c·∫ßu ·∫°."
                detected_intent = "Angry/Handover"
                end_of_mvp = True; session['step'] = 0
                
                # [FIX] G·ª≠i Metrics TR∆Ø·ªöC Text
                yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)

                response_text = self._normalize_pronouns(response_text, current_pronoun)
                async for chunk in self._stream_text_and_audio(response_text): yield chunk
                bot_reply_accumulated = response_text

            # === NH√ÅNH 3: T·ª™ CH·ªêI ===
            elif re.search(r"(kh√¥ng\s+c·∫ßn|kh√¥ng\s+mu·ªën|kh√¥ng\s+ƒëƒÉng\s+k√Ω|th√¥i\s+em)", user_text.lower()):
                print(f" [INTENT] Kh√°ch t·ª´ ch·ªëi -> K·∫øt th√∫c.")
                response_text = f"D·∫° v√¢ng ·∫°, em c·∫£m ∆°n {current_pronoun.lower()} ƒë√£ d√†nh th·ªùi gian trao ƒë·ªïi. Em ch√†o {current_pronoun.lower()} ·∫°."
                end_of_mvp = True; session['step'] = 0
                
                # [FIX] G·ª≠i Metrics TR∆Ø·ªöC Text
                yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)

                response_text = self._normalize_pronouns(response_text, current_pronoun)
                async for chunk in self._stream_text_and_audio(response_text): yield chunk
                bot_reply_accumulated = response_text

            else:
                detected = self.analyzer.classify_issue(user_text)
                issue = detected[0].split(" ('")[0] if detected else None
                
                if issue: 
                    detected_intent = issue
                    session['detected_intent'] = issue

                # === NH√ÅNH 4: STREAMING STRATEGY ===
                if issue and issue in self.strategies:
                    print(f" [PIPELINE] K√≠ch ho·∫°t Streaming Strategy: {issue}")
                    if step == 1: session['step'] = 2

                    stream_gen = self.strategies[issue].execute_stream_gen(cid, user_text)
                    buffer = ""
                    
                    # [OK] G·ª≠i Metrics TR∆Ø·ªöC Text (ƒê√£ c√≥ s·∫µn ·ªü b·∫£n tr∆∞·ªõc)
                    yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)

                    async for chunk_text in stream_gen:
                        buffer += chunk_text
                        if " " in chunk_text or len(buffer) > 10:
                             yield json.dumps({"bot_text": chunk_text}) + "\n"

                        parts = self.sentence_split_regex.split(buffer)
                        if len(parts) > 1:
                            for i in range(len(parts) - 1):
                                sentence = parts[i].strip()
                                if sentence:
                                    sentence = self._normalize_pronouns(sentence, current_pronoun)
                                    bot_reply_accumulated += sentence + " "
                                    print(f"   üåä [Flow] G·ª≠i TTS c√¢u: {sentence[:30]}...")
                                    async for audio_chunk in self._stream_audio_only(sentence): yield audio_chunk
                            buffer = parts[-1]

                    if buffer.strip():
                        buffer = self._normalize_pronouns(buffer, current_pronoun)
                        bot_reply_accumulated += buffer
                        print(f"   üåä [Flow] G·ª≠i TTS c√¢u cu·ªëi: {buffer[:30]}...")
                        async for chunk in self._stream_audio_only(buffer): yield chunk

                # === NH√ÅNH 5: K·ªäCH B·∫¢N Tƒ®NH (SCRIPT GREETING) ===
                elif step == 2 and self.sales_data.get(cid):
                    response_text = self.sales_data.get(cid)['script_greeting']
                    
                    # [FIX] G·ª≠i Metrics TR∆Ø·ªöC Text
                    yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)

                    response_text = self._normalize_pronouns(response_text, current_pronoun)
                    async for chunk in self._stream_text_and_audio(response_text): yield chunk
                    bot_reply_accumulated = response_text
                
                # === NH√ÅNH 6: FALLBACK / COMPETITOR (NON-STREAM) ===
                else:
                    if "ƒë·∫Øt" in user_text.lower() or "gi√°" in user_text.lower():
                        response_text = await asyncio.to_thread(self.strategies["ƒê·ªëi th·ªß"].execute, cid, user_text)
                    else:
                        print(" [MVP] Out of scope -> Fallback Message.")
                        response_text = self.MSG_FALLBACK
                        detected_intent = "Fallback/Out of Scope"
                    
                    # [FIX] G·ª≠i Metrics TR∆Ø·ªöC Text
                    yield self._create_metrics_payload(stt_latency, start_process, sentiment_score, detected_intent)

                    response_text = self._normalize_pronouns(response_text, current_pronoun)
                    async for chunk in self._stream_text_and_audio(response_text): yield chunk
                    bot_reply_accumulated = response_text

        print(f" [BOT FINAL] {bot_reply_accumulated[:100]}...")

        # --- [QUAN TR·ªåNG] L∆ØU D·ªÆ LI·ªÜU V√ÄO DB ---
        if end_of_mvp:
            print(f" [DB] ƒêang l∆∞u d·ªØ li·ªáu cu·ªôc g·ªçi cho ID {cid}...")
            
            session_start = session.get('start_time', time.time() - 5)
            duration = time.time() - session_start
            if duration < 1: duration = 1 

            is_ai_resolved = True if detected_intent != "Angry/Handover" else False
            is_upsell = True if is_agreed else False
            cost_vnd = int(1000 + (duration * 50))
            
            sent_label = "neutral"
            if sentiment_score >= 0.4: sent_label = "positive"
            elif sentiment_score <= -0.4: sent_label = "negative"
            csat_val = 5 if sent_label == "positive" else (2 if sent_label == "negative" else 4)

            intent_mapping = {
                "M·∫°ng ngh·∫Ωn": "network_issue",
                "M·∫°ng y·∫øu": "network_issue",
                "√çt data": "low_data",
                "H·∫øt dung l∆∞·ª£ng": "low_data",
                "Dung l∆∞·ª£ng": "low_data",
                "H·ªßy g√≥i": "cancel_package",
                "ƒê·ªëi th·ªß": "competitor",
                "Gi√° cao": "competitor",
                "Buying Signal": "network_issue", 
                "General": "network_issue",
                "Angry/Handover": "cancel_package"
            }
            final_intent = intent_mapping.get(detected_intent, "network_issue")

            try:
                db.add_call(
                    customer_id=cid,
                    duration=int(duration),
                    intent=final_intent, 
                    sentiment=sent_label,
                    ai_resolved=is_ai_resolved,
                    upsell=is_upsell,
                    cost={'value': cost_vnd, 'csat': csat_val}
                )
                print(f" [DB] L∆∞u th√†nh c√¥ng.")
            except Exception as e:
                print(f" [DB ERROR] {e}")

            print(f" [MVP END] G·ª≠i th√¥ng b√°o m√†n h√¨nh: {self.MSG_MVP_END}")
            yield json.dumps({ "bot_text": self.MSG_MVP_END, "end_session": True }) + "\n"

    # ... (C√°c h√†m helper gi·ªØ nguy√™n) ...
    def _normalize_pronouns(self, text, target_pronoun):
        candidates = ["qu√Ω kh√°ch", "b·∫°n", "m√¨nh", "anh", "ch·ªã", "c√°c b·∫°n", "kh√°ch h√†ng", "kh√°ch"]
        for word in candidates:
            if word.lower() == target_pronoun.lower(): continue
            pattern = r'\b' + re.escape(word) + r'\b'
            text = re.sub(pattern, target_pronoun, text, flags=re.IGNORECASE)
        for _ in range(3):
            pattern = r'(?i)\b' + re.escape(target_pronoun) + r'\s*(?:[.,;]\s*)?' + re.escape(target_pronoun) + r'\b'
            text = re.sub(pattern, target_pronoun, text)
        text = text.strip()
        if text and text.lower().startswith(target_pronoun.lower()):
            text = target_pronoun.capitalize() + text[len(target_pronoun):]
        return text

    async def _stream_text_and_audio(self, full_text):
        yield json.dumps({"bot_text": full_text}) + "\n"
        sentences = re.split(r'(?<=[.?!])\s+', full_text)
        buffer = ""
        for s in sentences:
            if not s.strip(): continue
            buffer += s + " "
            if len(buffer) > 10 or s == sentences[-1]:
                async for audio_chunk in self._stream_audio_only(buffer): yield audio_chunk
                buffer = ""

    async def _stream_audio_only(self, text):
        if not text.strip(): return
        try:
            audio_bytes = await self.ai_service.text_to_speech(text)
            if audio_bytes:
                b64 = base64.b64encode(audio_bytes).decode("utf-8")
                yield json.dumps({"audio_base64": b64}) + "\n"
        except: pass

agent = TelesalesAgent()